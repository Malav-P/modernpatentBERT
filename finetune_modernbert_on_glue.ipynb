{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with ModernBERT & GLUE\n",
    "\n",
    "Created by: [Wayde Gilliam](https://twitter.com/waydegilliam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders Strike Back!\n",
    "\n",
    "Like many, I have fond memories of finetuning deberta, roberta and bert models for a number of Kaggle comps and real-world problems (e.g., NER, sentiment analysis, etc.).  Encoder models were \"the thing\" back in the day and continue to be the primary workhorse for many ML pipelines today though they have been eclipsed by recent advancements in LLMs which typically are based on decoder-only architectures. Long have we awaited a return to an encoder model for the modern world. With ModernBERT, that wait is over! ModernBERT is a new encoder-only model that incorporates the latest features in making neural networks more efficient, faster, and better at handling tasks that encoder models have long excelled at such at text classification.  In addition, ModernBERT allows us to break out of that max 512 token limit with their long context capabilities which give us 8,192 tokens to play with.\n",
    "\n",
    "In this tutorial, we'll go through the steps of fine-tuning ModernBERT for one of the GLUE tasks, MRPC.  We'll cover some key settings required to use it with the HuggingFace trainer and include with some recommended hyperparameters that have served us well in fine-tuning ModernBERT for GLUE.  We'll also see how to use the model for inference and cleanup the model from the GPU to free up resources.\n",
    "\n",
    "As an aside, I'm running all this code on a single 3090 with plenty of GPU memory to spare.\n",
    "\n",
    "Though not strictly necessary, **ModernBERT trains better with FlashAttention!**. Training and inference will be much faster with it installed. See below:\n",
    "\n",
    "ModernBERT is built on top of FlashAttention which is a highly optimized implementation of the attention mechanism that is faster and more memory efficient than the standard implementation.  ***The beauty of this is all you need to do is install it for ModernBERT to work with it!***  Here's how ...\n",
    "\n",
    "For NVIDIA GPUs with compute capability 8.0+ (Ampere/Ada/Hopper architecture - A100, A6000, RTX 3090, RTX 4090, H100 etc):\n",
    "```python\n",
    "pip install flash-attn --no-build-isolation\n",
    "```\n",
    "\n",
    "For older NVIDIA GPUs (pre-Ampere):\n",
    "```python\n",
    "pip install flash-attn --no-deps\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start jupyter server\n",
    "# salloc -G A100:1\n",
    "\n",
    "# conda activate mberft\n",
    "# jupyter notebook --ip=0.0.0.0 --port=8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True  # Suppresses compilation errors and falls back to eager mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "patents = load_dataset(\"MalavP/USPTO-3M\",split=\"train\", use_auth_token=hf_token)\n",
    "# Split the dataset: 90% for \"dummy\" (discarded), 10% for the subset\n",
    "mini_patents = patents.train_test_split(\n",
    "    test_size=0.01,  # 10% of the original data\n",
    "    shuffle=True,   # Randomize selection\n",
    "    seed=42         # For reproducibility\n",
    ")[\"test\"]           # Keep the 10% test split\n",
    "\n",
    "mini_patents = mini_patents.rename_column(\"cpc_ids\", \"labels\")\n",
    "mini_patents = mini_patents.map(lambda x: {'labels': x['labels'].split(',')})\n",
    "unique_labels = set(item for sublist in mini_patents[\"labels\"] for item in sublist)\n",
    "\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder on all the possible string labels\n",
    "label_encoder.fit(list(unique_labels))\n",
    "\n",
    "def convert_labels(example):\n",
    "    indices =  [label_encoder.transform([label])[0] for label in example['labels']]\n",
    "    example[\"labels\"] = [float(i in indices) for i in range(len(unique_labels))]\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the transformation to the dataset\n",
    "mini_patents = mini_patents.map(convert_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = mini_patents.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]\n",
    "# Verify the size\n",
    "print(f\"Original size: {len(patents)}, Subset size: {len(mini_patents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model id to load the tokenizer\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "# model_id = \"google-bert/bert-base-uncased\"\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.model_max_length = 512 # set model_max_length to 512 as prompts are not longer than 1024 tokens\n",
    " \n",
    "# Tokenize helper function\n",
    "def tokenize(batch):\n",
    "    # return tokenizer(batch['text'], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    return tokenizer(batch['text'], truncation=True, padding=True, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Load the model for sequence classification (adjust num_labels as needed).\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(unique_labels)).to(device)\n",
    "model.config.problem_type= \"multi_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Define a compute_metrics function to compute accuracy.\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred # shape (N, num classes) and shape (N, num classes)\n",
    "    pred_labels = np.argmax(predictions, axis=-1)  # shape (N,)\n",
    "\n",
    "    # Assuming labels are lists of labels, check if pred_labels are in any of the true labels\n",
    "    correct = 0\n",
    "    total = len(labels)\n",
    "\n",
    "    for pred, true_labels in zip(pred_labels, labels):\n",
    "        correct += true_labels[pred]\n",
    "\n",
    "    # Accuracy calculation\n",
    "    accuracy = correct / total\n",
    "    return {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your hyperparameters and task identifier\n",
    "train_bsz, val_bsz = 32, 32\n",
    "lr = 8e-5 # the authors use 2e-5\n",
    "betas = (0.9, 0.98)\n",
    "n_epochs = 1\n",
    "eps = 1e-6\n",
    "wd = 8e-6\n",
    "task = \"your_task_name\"  # Set this to an appropriate identifier for your task\n",
    "# 8. Define training arguments using your specified hyperparameters.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"aai_ModernBERT_{task}_ft\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=train_bsz,\n",
    "    per_device_eval_batch_size=val_bsz,\n",
    "    num_train_epochs=n_epochs,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=betas[0],\n",
    "    adam_beta2=betas[1],\n",
    "    adam_epsilon=eps,\n",
    "    weight_decay=wd,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    push_to_hub=False,\n",
    "    disable_tqdm=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. Create the Trainer with the compute_metrics function.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=hf_data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_token = os.getenv(\"WANDB_TOKEN\")\n",
    "if wandb_token is not None:\n",
    "    import wandb\n",
    "    wandb_token = os.getenv(\"WANDB_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33maai_ModernBERT_your_task_name_ft\u001b[0m at: \u001b[34mhttps://wandb.ai/gauthierroy1-gt/huggingface/runs/emd4b8km\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35m../../../../../../storage/ice1/1/4/mpatel636/cs7643/ModernBERT/wandb/run-20250217_180520-emd4b8km/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GLUE?\n",
    "\n",
    "The [General Language Understanding Evaluation (GLUE) benchmark](https://gluebenchmark.com/) is a collection of nine diverse natural language understanding tasks designed to evaluate and compare the performance of NLP models across various language comprehension challenges. By providing a standardized framework, GLUE facilitates the development of models that generalize well across multiple tasks, promoting advancements in creating robust and versatile language understanding systems. \n",
    "\n",
    "Let's put this all these tasks in a dictionary along with some other helpful metadata about each one that might prove useful to iteratting over all of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_tasks = {\n",
    "    \"cola\": {\n",
    "        \"abbr\": \"CoLA\",\n",
    "        \"name\": \"Corpus of Linguistic Acceptability\",\n",
    "        \"description\": \"Predict whether a sequence is a grammatical English sentence\",\n",
    "        \"task_type\": \"Single-Sentence Task\",\n",
    "        \"domain\": \"Misc.\",\n",
    "        \"size\": \"8.5k\",\n",
    "        \"metrics\": \"Matthews corr.\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation\", \"test\": \"test\"},\n",
    "        \"inputs\": [\"sentence\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [matthews_corrcoef],\n",
    "        \"n_labels\": 2,\n",
    "    },\n",
    "    \"sst2\": {\n",
    "        \"abbr\": \"SST-2\",\n",
    "        \"name\": \"Stanford Sentiment Treebank\",\n",
    "        \"description\": \"Predict the sentiment of a given sentence\",\n",
    "        \"task_type\": \"Single-Sentence Task\",\n",
    "        \"domain\": \"Movie reviews\",\n",
    "        \"size\": \"67k\",\n",
    "        \"metrics\": \"Accuracy\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation\", \"test\": \"test\"},\n",
    "        \"inputs\": [\"sentence\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [accuracy_score],\n",
    "        \"n_labels\": 2,\n",
    "    },\n",
    "    \"mrpc\": {\n",
    "        \"abbr\": \"MRPC\",\n",
    "        \"name\": \"Microsoft Research Paraphrase Corpus\",\n",
    "        \"description\": \"Predict whether two sentences are semantically equivalent\",\n",
    "        \"task_type\": \"Similarity and Paraphrase Tasks\",\n",
    "        \"domain\": \"News\",\n",
    "        \"size\": \"3.7k\",\n",
    "        \"metrics\": \"F1/Accuracy\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation\", \"test\": \"test\"},\n",
    "        \"inputs\": [\"sentence1\", \"sentence2\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [accuracy_score, f1_score],\n",
    "        \"n_labels\": 2,\n",
    "    },\n",
    "    \"stsb\": {\n",
    "        \"abbr\": \"SST-B\",\n",
    "        \"name\": \"Semantic Textual Similarity Benchmark\",\n",
    "        \"description\": \"Predict the similarity score for two sentences on a scale from 1 to 5\",\n",
    "        \"task_type\": \"Similarity and Paraphrase Tasks\",\n",
    "        \"domain\": \"Misc.\",\n",
    "        \"size\": \"7k\",\n",
    "        \"metrics\": \"Pearson/Spearman corr.\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation\", \"test\": \"test\"},\n",
    "        \"inputs\": [\"sentence1\", \"sentence2\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [pearsonr, spearmanr],\n",
    "        \"n_labels\": 1,\n",
    "    },\n",
    "    \"qqp\": {\n",
    "        \"abbr\": \"QQP\",\n",
    "        \"name\": \"Quora question pair\",\n",
    "        \"description\": \"Predict if two questions are a paraphrase of one another\",\n",
    "        \"task_type\": \"Similarity and Paraphrase Tasks\",\n",
    "        \"domain\": \"Social QA questions\",\n",
    "        \"size\": \"364k\",\n",
    "        \"metrics\": \"F1/Accuracy\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation\", \"test\": \"test\"},\n",
    "        \"inputs\": [\"question1\", \"question2\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [f1_score, accuracy_score],\n",
    "        \"n_labels\": 2,\n",
    "    },\n",
    "    \"mnli-matched\": {\n",
    "        \"abbr\": \"MNLI\",\n",
    "        \"name\": \"Mulit-Genre Natural Language Inference\",\n",
    "        \"description\": \"Predict whether the premise entails, contradicts or is neutral to the hypothesis\",\n",
    "        \"task_type\": \"Inference Tasks\",\n",
    "        \"domain\": \"Misc.\",\n",
    "        \"size\": \"393k\",\n",
    "        \"metrics\": \"Accuracy\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation_matched\", \"test\": \"test_matched\"},\n",
    "        \"inputs\": [\"premise\", \"hypothesis\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [accuracy_score],\n",
    "        \"n_labels\": 3,\n",
    "    },\n",
    "    \"mnli-mismatched\": {\n",
    "        \"abbr\": \"MNLI\",\n",
    "        \"name\": \"Mulit-Genre Natural Language Inference\",\n",
    "        \"description\": \"Predict whether the premise entails, contradicts or is neutral to the hypothesis\",\n",
    "        \"task_type\": \"Inference Tasks\",\n",
    "        \"domain\": \"Misc.\",\n",
    "        \"size\": \"393k\",\n",
    "        \"metrics\": \"Accuracy\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation_mismatched\", \"test\": \"test_mismatched\"},\n",
    "        \"inputs\": [\"premise\", \"hypothesis\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [accuracy_score],\n",
    "        \"n_labels\": 3,\n",
    "    },\n",
    "    \"qnli\": {\n",
    "        \"abbr\": \"QNLI\",\n",
    "        \"name\": \"Stanford Question Answering Dataset\",\n",
    "        \"description\": \"Predict whether the context sentence contains the answer to the question\",\n",
    "        \"task_type\": \"Inference Tasks\",\n",
    "        \"domain\": \"Wikipedia\",\n",
    "        \"size\": \"105k\",\n",
    "        \"metrics\": \"Accuracy\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation\", \"test\": \"test\"},\n",
    "        \"inputs\": [\"question\", \"sentence\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [accuracy_score],\n",
    "        \"n_labels\": 2,\n",
    "    },\n",
    "    \"rte\": {\n",
    "        \"abbr\": \"RTE\",\n",
    "        \"name\": \"Recognize Textual Entailment\",\n",
    "        \"description\": \"Predict whether one sentece entails another\",\n",
    "        \"task_type\": \"Inference Tasks\",\n",
    "        \"domain\": \"News, Wikipedia\",\n",
    "        \"size\": \"2.5k\",\n",
    "        \"metrics\": \"Accuracy\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation\", \"test\": \"test\"},\n",
    "        \"inputs\": [\"sentence1\", \"sentence2\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [accuracy_score],\n",
    "        \"n_labels\": 2,\n",
    "    },\n",
    "    \"wnli\": {\n",
    "        \"abbr\": \"WNLI\",\n",
    "        \"name\": \"Winograd Schema Challenge\",\n",
    "        \"description\": \"Predict if the sentence with the pronoun substituted is entailed by the original sentence\",\n",
    "        \"task_type\": \"Inference Tasks\",\n",
    "        \"domain\": \"Fiction books\",\n",
    "        \"size\": \"634\",\n",
    "        \"metrics\": \"Accuracy\",\n",
    "        \"dataset_names\": {\"train\": \"train\", \"valid\": \"validation\", \"test\": \"test\"},\n",
    "        \"inputs\": [\"sentence1\", \"sentence2\"],\n",
    "        \"target\": \"label\",\n",
    "        \"metric_funcs\": [accuracy_score],\n",
    "        \"n_labels\": 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "# for v in glue_tasks.values(): print(v)\n",
    "glue_tasks.values()\n",
    "\n",
    "glue_df = pd.DataFrame(glue_tasks.values(), columns=[\"abbr\", \"name\", \"task_type\", \"description\", \"size\", \"metrics\"])\n",
    "glue_df.columns = glue_df.columns.str.replace(\"_\", \" \").str.capitalize()\n",
    "display(glue_df.style.set_properties(**{\"text-align\": \"left\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Fine-Tune ModernBERT for MRPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "ModernBERT currently comes in two flavors, base and large. To keep things lean and mean, we'll use the \"answerdotai/ModernBERT-base\" checkpoint for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mrpc\"\n",
    "task_meta = glue_tasks[task]\n",
    "train_ds_name = task_meta[\"dataset_names\"][\"train\"]\n",
    "valid_ds_name = task_meta[\"dataset_names\"][\"valid\"]\n",
    "test_ds_name = task_meta[\"dataset_names\"][\"test\"]\n",
    "\n",
    "task_inputs = task_meta[\"inputs\"]\n",
    "task_target = task_meta[\"target\"]\n",
    "n_labels = task_meta[\"n_labels\"]\n",
    "task_metrics = task_meta[\"metric_funcs\"]\n",
    "\n",
    "checkpoint = \"answerdotai/ModernBERT-base\"  # \"answerdotai/ModernBERT-base\", \"answerdotai/ModernBERT-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `Datasets` library to load the data.  As its always recommended to \"look at your data\" before we get training, we'll also print out a single example to see what we're working with as well as the features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"glue\", task)\n",
    "\n",
    "print(f\"{raw_datasets}\\n\")\n",
    "print(f\"{raw_datasets[train_ds_name][0]}\\n\")\n",
    "print(f\"{raw_datasets[train_ds_name].features}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following dictionaries when building our model with `AutoModelForSequenceClassification` to map between the label ids and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_maps(raw_datasets, train_ds_name):\n",
    "    labels = raw_datasets[train_ds_name].features[\"label\"]\n",
    "\n",
    "    id2label = {idx: name.upper() for idx, name in enumerate(labels.names)} if hasattr(labels, \"names\") else None\n",
    "    label2id = {name.upper(): idx for idx, name in enumerate(labels.names)} if hasattr(labels, \"names\") else None\n",
    "\n",
    "    return id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label, label2id = get_label_maps(raw_datasets, train_ds_name)\n",
    "\n",
    "print(f\"{id2label}\")\n",
    "print(f\"{label2id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRPC is a sentence-pair classification task where we're given two sentences and asked to predict whether they are paraphrases of one another.  The dataset is split into train, validation and test sets. We'll need to keep all this in mind when we set up tokenization next with `AutoTokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Next we define our Tokenizer and a preprocess function to create the input_ids, attention_mask, and token_type_ids the model nees to train.  For this example, including `truncation=True` is enough as we'll rely on our data collation function below to put our batches into the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, task_inputs):\n",
    "    inps = [examples[inp] for inp in task_inputs]\n",
    "    tokenized = hf_tokenizer(*inps, truncation=True)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(partial(preprocess_function, task_inputs=task_inputs), batched=True)\n",
    "\n",
    "print(f\"{tokenized_datasets}\\n\")\n",
    "print(f\"{tokenized_datasets[train_ds_name][0]}\\n\")\n",
    "print(f\"{tokenized_datasets[train_ds_name].features}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good to see what the tokenizer is doing to our data to ensure the special tokens are where we expect them to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer.decode(tokenized_datasets[train_ds_name][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our `task_metrics` to compute the metrics for our model.  We'll return a dictionary of the metric name and value for each metric we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred, task_metrics):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    metrics_d = {}\n",
    "    for metric_func in task_metrics:\n",
    "        metric_name = metric_func.__name__\n",
    "        if metric_name in [\"pearsonr\", \"spearmanr\"]:\n",
    "            score = metric_func(labels, np.squeeze(predictions))\n",
    "        else:\n",
    "            score = metric_func(np.argmax(predictions, axis=-1), labels)\n",
    "\n",
    "        if isinstance(score, tuple):\n",
    "            metrics_d[metric_func.__name__] = score[0]\n",
    "        else:\n",
    "            metrics_d[metric_func.__name__] = score\n",
    "\n",
    "    return metrics_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "This is where the fun begins! Here we setup a few hyperparameters than have proven to work well for us in fine-tuning ModernBERT-base on GLUE tasks.  We'll also setup our model, data collator, and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bsz, val_bsz = 32, 32\n",
    "lr = 8e-5\n",
    "betas = (0.9, 0.98)\n",
    "n_epochs = 2\n",
    "eps = 1e-6\n",
    "wd = 8e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When configuring `AutoModelForSequenceClassification`, two settings are critical to get things working with the HuggingFace `Trainer`. One is the `num_labels` we're expecting and the other is to set `compile=False` to avoid using the `torch.compile` function which is not supported in Transformers at the time of this writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=n_labels, id2label=id2label, label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collation is easy for GLUE tasks as we can use the `DataCollatorWithPadding` class to pad our input_ids and attention_mask to the max length in the batch.\n",
    "\n",
    "**Note**: If you have installed Flash Attention, ModernBERT removes the padding internally, which makes it the fastest version. SPDA and Eager mode will be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_data_collator = DataCollatorWithPadding(tokenizer=hf_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in place, we can now setup our `TrainingArguments` and `Trainer` and get to training! Lots of customization is possible here and it is recommended to play with different schedulers and the hyperparameters we've started y'all off with above to improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"aai_ModernBERT_{task}_ft\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=train_bsz,\n",
    "    per_device_eval_batch_size=val_bsz,\n",
    "    num_train_epochs=n_epochs,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=betas[0],\n",
    "    adam_beta2=betas[1],\n",
    "    adam_epsilon=eps,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `TrainerCallback` so that we can capture all the training and evaluation logs and store them for later analysis. By default, the `Trainer` class will only keep the latest logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_history = {\"train\": [], \"eval\": []}\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            if \"loss\" in logs:  # Training logs\n",
    "                self.training_history[\"train\"].append(logs)\n",
    "            elif \"eval_loss\" in logs:  # Evaluation logs\n",
    "                self.training_history[\"eval\"].append(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=hf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[train_ds_name],\n",
    "    eval_dataset=tokenized_datasets[valid_ds_name],\n",
    "    processing_class=hf_tokenizer,\n",
    "    data_collator=hf_data_collator,\n",
    "    compute_metrics=partial(compute_metrics, task_metrics=task_metrics),\n",
    ")\n",
    "\n",
    "metrics_callback = MetricsCallback()\n",
    "trainer.add_callback(metrics_callback)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "train_history_df = pd.DataFrame(metrics_callback.training_history[\"train\"])\n",
    "train_history_df = train_history_df.add_prefix(\"train_\")\n",
    "eval_history_df = pd.DataFrame(metrics_callback.training_history[\"eval\"])\n",
    "train_res_df = pd.concat([train_history_df, eval_history_df], axis=1)\n",
    "\n",
    "args_df = pd.DataFrame([training_args.to_dict()])\n",
    "\n",
    "display(train_res_df)\n",
    "display(args_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "There'a number of options for inference within the HuggingFace ecosystem.  We'll go a bit old school here and just use the `forward` method of the model. We're not uploading this model to the hub, but this is an easy enough task for you to try out on your own should you like to share your ModernBERT finetune :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "ex_2 = \"I love lamp!\"\n",
    "\n",
    "inf_inputs = hf_tokenizer(ex_1, ex_2, return_tensors=\"pt\")\n",
    "inf_inputs = inf_inputs.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = hf_model(**inf_inputs).logits\n",
    "\n",
    "print(logits)\n",
    "print(f\"Prediction: {hf_model.config.id2label[logits.argmax().item()]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(things_to_delete: list | None = None):\n",
    "    if things_to_delete is not None:\n",
    "        for thing in things_to_delete:\n",
    "            if thing is not None:\n",
    "                del thing\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup(things_to_delete=[hf_model, trainer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all the GLUE!\n",
    "\n",
    "If you got this far you're probably wondering why I put together that dictionary of GLUE tasks if all we're doing is finetuning a single model. The answer is basically that I'm a good and lazy programmer who would like to easily run hyperparameter sweeps and/or fine-tunes on all the GLUE tasks. So ... let's do that!\n",
    "\n",
    "We'll run with the training hyperparameters specified above and I leave it to the reader to improve the method below to be able to override these values should folks be looking for something to do :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_glue_task(\n",
    "    task: str, checkpoint: str = \"answerdotai/ModernBERT-base\", train_subset: int | None = None, do_cleanup: bool = True\n",
    "):  # 1. Load the task metadata\n",
    "    task_meta = glue_tasks[task]\n",
    "    train_ds_name = task_meta[\"dataset_names\"][\"train\"]\n",
    "    valid_ds_name = task_meta[\"dataset_names\"][\"valid\"]\n",
    "\n",
    "    task_inputs = task_meta[\"inputs\"]\n",
    "    n_labels = task_meta[\"n_labels\"]\n",
    "    task_metrics = task_meta[\"metric_funcs\"]\n",
    "\n",
    "    # 2. Load the dataset\n",
    "    raw_datasets = load_dataset(\"glue\", task.split(\"-\")[0] if \"-\" in task else task)\n",
    "    if train_subset is not None and len(raw_datasets[\"train\"]) > train_subset:\n",
    "        raw_datasets[\"train\"] = raw_datasets[\"train\"].shuffle(seed=42).select(range(train_subset))\n",
    "\n",
    "    id2label, label2id = get_label_maps(raw_datasets, train_ds_name)\n",
    "\n",
    "    # 3. Load the tokenizer\n",
    "    hf_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    tokenized_datasets = raw_datasets.map(partial(preprocess_function, task_inputs=task_inputs), batched=True)\n",
    "\n",
    "    # 4. Define the compute metrics function\n",
    "    task_compute_metrics = partial(compute_metrics, task_metrics=task_metrics)\n",
    "\n",
    "    # 5. Load the model and data collator\n",
    "    model_additional_kwargs = {\"id2label\": id2label, \"label2id\": label2id} if id2label and label2id else {}\n",
    "    hf_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint, num_labels=n_labels, compile=False, **model_additional_kwargs\n",
    "    )\n",
    "\n",
    "    hf_data_collator = DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "\n",
    "    # 6. Define the training arguments and trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"aai_ModernBERT_{task}_ft\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=train_bsz,\n",
    "        per_device_eval_batch_size=val_bsz,\n",
    "        num_train_epochs=n_epochs,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        optim=\"adamw_torch\",\n",
    "        adam_beta1=betas[0],\n",
    "        adam_beta2=betas[1],\n",
    "        adam_epsilon=eps,\n",
    "        logging_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        bf16=True,\n",
    "        bf16_full_eval=True,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=hf_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[train_ds_name],\n",
    "        eval_dataset=tokenized_datasets[valid_ds_name],\n",
    "        processing_class=hf_tokenizer,\n",
    "        data_collator=hf_data_collator,\n",
    "        compute_metrics=task_compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Add callback to trainer\n",
    "    metrics_callback = MetricsCallback()\n",
    "    trainer.add_callback(metrics_callback)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 7. Get the training results and hyperparameters\n",
    "    train_history_df = pd.DataFrame(metrics_callback.training_history[\"train\"])\n",
    "    train_history_df = train_history_df.add_prefix(\"train_\")\n",
    "    eval_history_df = pd.DataFrame(metrics_callback.training_history[\"eval\"])\n",
    "    train_res_df = pd.concat([train_history_df, eval_history_df], axis=1)\n",
    "\n",
    "    args_df = pd.DataFrame([training_args.to_dict()])\n",
    "\n",
    "    # 8. Cleanup (optional)\n",
    "    if do_cleanup:\n",
    "        cleanup(things_to_delete=[trainer, hf_model, hf_tokenizer, tokenized_datasets, raw_datasets])\n",
    "\n",
    "    return train_res_df, args_df, hf_model, hf_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helpful function encapsulates all the steps we've been through above and allows us to easily run a fine-tune on a single task. In addition to the HuggingFace objects, it returns the training results, training hyperparameters (all potentially helpful for performing sweeps and or documenting your results).\n",
    "\n",
    "Let's give it a go on both MRPC and CoLA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_df, args_df, hf_model, hf_tokenizer = finetune_glue_task(\n",
    "    \"mrpc\", checkpoint=\"answerdotai/ModernBERT-base\", do_cleanup=True\n",
    ")\n",
    "\n",
    "display(train_res_df)\n",
    "display(args_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_df, args_df, hf_model, hf_tokenizer = finetune_glue_task(\n",
    "    \"cola\", checkpoint=\"answerdotai/ModernBERT-base\", do_cleanup=True\n",
    ")\n",
    "\n",
    "display(train_res_df)\n",
    "display(args_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Send it!**\n",
    "\n",
    "Grab yourself a good cup of coffee, take your pups out for a walk, or whatever as your GPU purrs along while finetuning all things GLUE!\n",
    "\n",
    "Note the `train_subset` parameter which allows us to train on a subset of the dataset. This is helpful for quickly testing the model on a small dataset to make sure all the bits work as expected.  Feel free to set it to `None` for a full send!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in glue_tasks.keys():\n",
    "    print(f\"----- Finetuning {task} -----\")\n",
    "    train_res_df, args_df, hf_model, hf_tokenizer = finetune_glue_task(\n",
    "        task, checkpoint=\"answerdotai/ModernBERT-base\", train_subset=1_000, do_cleanup=True\n",
    "    )\n",
    "\n",
    "    print(\":: Results ::\")\n",
    "    display(train_res_df)\n",
    "    display(args_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With ModernBERT encoders are back baby!  We've seen that ModernBERT-base can compete with the best of them on GLUE tasks and with a little more tuning, we'll see that ModernBERT-large can do even better.  I'm excited to see what the community will do with this model and I'm looking forward to seeing what you all build with it! We'll be exploring more of the capabilities of ModernBERT in future tutorials.\n",
    "\n",
    "Until next time, happy coding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
